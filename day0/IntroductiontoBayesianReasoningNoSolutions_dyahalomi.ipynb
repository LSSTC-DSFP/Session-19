{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "431bdde7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A (brief) Introduction to Bayesian Reasoning and Statistics\n",
    "\n",
    "#### Bryan Scott, CIERA/Northwestern, \n",
    "#### DSFP Session 19: Orientation for Cohort 7 \n",
    "version 0.1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b570ae7f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1: Warm Up - Conditional Probability and the Law of Total Probability\n",
    "\n",
    "One of the main objects of Bayesian inference is the conditional probability distribution. This tells us what the chance of something happening given that we know something else happened are. For example,\n",
    "\n",
    "$$\n",
    "p(Good | Dog) = 1 \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235467a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A related concept is the law of total probability - this just formalizes the idea that the odds of something happening must involve a sum over all of the (conditional) ways it can happen. \n",
    "\n",
    "$$\n",
    "p(B) = \\Sigma_n P(B | A_n) P(A_n)\n",
    "$$\n",
    "\n",
    "for example,\n",
    "\n",
    "$$\n",
    "p(Pet) = p(Pet| Own) p(Own) + p(Pet| Friendly) p(Friendly)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e863d8b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One more useful bit of probability theory is that conditional probabilities can be related to each other via a simple relationship:\n",
    "\n",
    "$$\n",
    "p(A| B) = \\frac{P(B|A)P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "which also holds if you swap $A \\rightarrow B$. This result is sometimes called Bayes' theorem, and the interpretation of this equation (which is simply a fact about conditional probabilities) gives rise to Bayesian statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d308dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 2: Prior Beliefs\n",
    "\n",
    "Let's consider the following problem. Suppose you're deciding whether to take the bus or walk to your destination. Unfortunately, you live in Chicago, so while there is a bus tracker app, it will tell you only a set of probable times you might expect the bus to arrive. Having previously solved this problem empirically, one learns not to rely on the bus tracker app's estimates. \n",
    "\n",
    "As such, you'll call the rate at which buses arrive $\\lambda$ and use the bus schedule to set a best guess $p_0(\\lambda)$ for the rate at which buses arrived. $\\lambda$ is a $\\textit{parameter}$ of some distribution we wish to infer. It's assumed distribution before we have observed any buses arriving, $p_0(\\lambda)$ is called the $\\textit{prior}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d89687a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 3: Conditioning Beliefs on Evidence\n",
    "\n",
    "Now that we know how often we expect the bus to arrive, we need someway to update our knowledge of $\\lambda$ based on observing bus arrival times. This is called the $\\textbf{likelihood}$. It tells us how our observed data changes our beliefs about the value of some parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aa62fc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "While choosing a distribution that captures our prior beliefs about the value of a parameter often requires care, there are more clear rules governing the proper updating functions. It turns out that the right updating function for this problem is the $\\textit{Poisson Distribution}$.\n",
    "\n",
    "For most problems, you will instead use a $\\textit{Gaussian distribution}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb144172",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 4: Bayes' Insight\n",
    "\n",
    "\n",
    "The deep insight is that Bayesian probabilities are statements about the probability of a parameter taking on a certain value. This differs from the classical interpretation, where probability is about realizations of the data, rather than distributions over parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ab2844",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example, what we care about as bus riders is the rate at which buses arrive. If we believe the next bus will arrive soon, we will likely wait for it. If we believe the bus will arrive much later, perhaps we will walk. What we want to quantify is our beliefs about the value of the rate parameter of bus arrivals. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068b5cbc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To make this quantitative, the probability distribution for the rate at which buses arrive $p(\\tau | \\lambda)$, conditioned on the time we've waited so far, is proportional to the product of our $\\textit{prior}$ belief about when a bus will arrive and the $\\textit{likelihood}$ - a function that tells us how our beliefs change as we observe (or don't observe) buses arriving. \n",
    "\n",
    "$$\n",
    "p(\\lambda | \\tau) \\propto p_0(\\lambda) p(\\tau | \\lambda )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eaab30",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There is a missing normalization constant here, sometimes called the $\\textbf{evidence}$ or the $\\textbf{marginal likelihood}$ - we will return to this point in detail in the Bayesian inference session, but for now, you can calculate the evidence by asserting that $\\Sigma_{\\tau} p(\\lambda | \\tau) = 1$. For most problems we can neglect this constant since we only care about the maximum of the probability distribution. When it does matter, we often use the law of total probability to calculate this term. More on this in a future session."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8841241e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What this equation says is that my $\\textit{belief}$ about the frequency at which buses arrive depends on my prior belief about how often the bus should come and a factor that tells me how to update my belief based on how long I've been waiting at the bus stop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967c18e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 5: Putting this into practice\n",
    "\n",
    "Now, in the rest of this notebook, work out the posterior estimate for the rate at which buses arrive based on your wait time so far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b79e6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4fae89",
   "metadata": {},
   "source": [
    "The choice of prior is, in general, a subtle problem that we will return to. In the analysis of wait times, it is common to select a prior from the $\\Gamma$ distribution family described by the equation:\n",
    "\n",
    "$$\n",
    "p_0 (\\lambda) = \\frac{\\beta^a \\lambda^{a-1} e^{-\\beta \\lambda}}{\\Gamma(a)}\n",
    "$$\n",
    "\n",
    "Begin by discussing with your partner what each of these parameters means in the context of a wait time problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06b8d20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c17524ed",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Next, either use a built in function from scipy or write your own function for the $\\Gamma(a)$ distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0423cf69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95302231",
   "metadata": {},
   "source": [
    "Next, we need to pick a likelihood function that updates our estimate of the rate based on observed wait times. We assume buses arrive as a Poisson process with a certain rate, in other words, our likelihood function is\n",
    "\n",
    "p(t | $\\lambda$) = $\\lambda t e^{-\\lambda t}$\n",
    "\n",
    "plot this as a function of $\\lambda$. You can use either a built in function or write your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4a4b97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69042209",
   "metadata": {},
   "source": [
    "Next, we use Bayes' theorem to calculate the posterior estimate of the rate at which buses arrive based on the time we have waited.\n",
    "\n",
    "p($\\lambda$ | t) $\\propto$ p(t|$\\lambda$) p($\\lambda$)\n",
    "\n",
    "(note that we are ignoring the denominator here - you can calculate it by summing over the right hand side for all values of $\\lambda$ - it is just a normalization constant.)\n",
    "\n",
    "Plot your posteriors as a function of the unknown parameter $\\lambda$ for many different values of the observed wait time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd82781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01b6af59",
   "metadata": {},
   "source": [
    "Based on your plots, answer the following questions: \n",
    "- What happens to your estimate of the rate at which buses arrive as a function of how long you've waited for the first bus? What does this imply about your estimate of the time until the next bus will arrive?\n",
    "- What does the shape of these distributions tell you about the $\\Gamma$ distribution and choice of Poisson likelihood for this problem. Is this behavior generalizable? \n",
    "\n",
    "(challenge) If you've answered the above questions, now solve this problem analytically and explicitly compute the expected time until the next bus arrives using your posterior distribution for the rate at which buses arrive. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
